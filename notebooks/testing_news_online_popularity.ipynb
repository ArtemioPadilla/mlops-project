{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2779ab39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Recarga automáticamente los módulos externos cuando cambian\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 1. Obtener la ruta del directorio raíz del proyecto\n",
    "# Sube un nivel desde el directorio actual del notebook (notebooks/)\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# 2. Agregar el directorio raíz al 'path' de Python\n",
    "if project_root not in sys.path:\n",
    "    print(f\"Agregando {project_root} al sys.path\")\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5c4ca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# --- Importacion de clase limpieza y eda\n",
    "from mlops_online_news_popularity.preprocess import cleaning_eda\n",
    "\n",
    "# --- Importaciones de Sklearn ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# CLASE: NewsPopularityModel\n",
    "# =====================================================================\n",
    "\n",
    "class NewsPopularityModel:\n",
    "    \"\"\"\n",
    "    Clase principal refactorizada.\n",
    "    \"\"\"\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.reports_path = '../docs/'\n",
    "        self.model_pipeline = None \n",
    "        self.data = None\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = [None] * 4\n",
    "        \n",
    "        self.expected_cols = [\n",
    "            \"url\",\"timedelta\",\"n_tokens_title\",\"n_tokens_content\",\"n_unique_tokens\",\n",
    "            \"n_non_stop_words\",\"n_non_stop_unique_tokens\",\"num_hrefs\",\"num_self_hrefs\",\n",
    "            \"num_imgs\",\"num_videos\",\"average_token_length\",\"num_keywords\",\n",
    "            \"data_channel_is_lifestyle\",\"data_channel_is_entertainment\",\"data_channel_is_bus\",\n",
    "            \"data_channel_is_socmed\",\"data_channel_is_tech\",\"data_channel_is_world\",\n",
    "            \"kw_min_min\",\"kw_max_min\",\"kw_avg_min\",\"kw_min_max\",\"kw_max_max\",\"kw_avg_max\",\n",
    "            \"kw_min_avg\",\"kw_max_avg\",\"kw_avg_avg\",\n",
    "            \"self_reference_min_shares\",\"self_reference_max_shares\",\"self_reference_avg_sharess\",\n",
    "            \"weekday_is_monday\",\"weekday_is_tuesday\",\"weekday_is_wednesday\",\"weekday_is_thursday\",\n",
    "            \"weekday_is_friday\",\"weekday_is_saturday\",\"weekday_is_sunday\",\"is_weekend\",\n",
    "            \"LDA_00\",\"LDA_01\",\"LDA_02\",\"LDA_03\",\"LDA_04\",\n",
    "            \"global_subjectivity\",\"global_sentiment_polarity\",\"global_rate_positive_words\",\n",
    "            \"global_rate_negative_words\",\"rate_positive_words\",\"rate_negative_words\",\n",
    "            \"avg_positive_polarity\",\"min_positive_polarity\",\"max_positive_polarity\",\n",
    "            \"avg_negative_polarity\",\"min_negative_polarity\",\"max_negative_polarity\",\n",
    "            \"title_subjectivity\",\"title_sentiment_polarity\",\"abs_title_subjectivity\",\n",
    "            \"abs_title_sentiment_polarity\",\"shares\"\n",
    "        ]\n",
    "        self.lda_cols = [\"LDA_00\",\"LDA_01\",\"LDA_02\",\"LDA_03\",\"LDA_04\"]\n",
    "        self.cols_to_drop = ['url', 'timedelta'] \n",
    "        self.TARGET_COL = 'shares'\n",
    "        self.threshold = 1400 \n",
    "\n",
    "    def load_data(self):\n",
    "        print(f\"Cargando datos desde {self.filepath}...\")\n",
    "        try:\n",
    "            self.data = pd.read_csv(self.filepath)\n",
    "            print(f\"Datos cargados exitosamente. Shape={self.data.shape}\")\n",
    "            DataExplorer.explore_data(self.data)\n",
    "            DataExplorer.generate_profiling_report(\n",
    "                self.data, \"Reporte 1: Datos Crudos (Raw)\", \n",
    "                self.reports_path, \"01_raw_data_report.html\"\n",
    "            )\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: No se encontró el archivo en {self.filepath}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error al cargar o explorar datos: {e}\")\n",
    "            return None\n",
    "        return self\n",
    "\n",
    "    def _handle_high_correlation(self, threshold=0.9):\n",
    "        \"\"\"\n",
    "        Encuentra y elimina características altamente correlacionadas del set de entrenamiento.\n",
    "        Actualiza self.X_train, self.X_test y self.cols_to_drop.\n",
    "        \"\"\"\n",
    "        print(f\"\\n--- Buscando características altamente correlacionadas (umbral > {threshold}) ---\")\n",
    "        \n",
    "        corr_matrix = self.X_train.corr(numeric_only=True).abs()\n",
    "        to_drop = set()\n",
    "        \n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i + 1, len(corr_matrix.columns)):\n",
    "                col_i = corr_matrix.columns[i]\n",
    "                col_j = corr_matrix.columns[j]\n",
    "                \n",
    "                if col_i in to_drop or col_j in to_drop:\n",
    "                    continue\n",
    "                    \n",
    "                if corr_matrix.iloc[i, j] > threshold:\n",
    "                    avg_corr_i = corr_matrix[col_i].mean()\n",
    "                    avg_corr_j = corr_matrix[col_j].mean()\n",
    "                    col_to_drop = col_i if avg_corr_i > avg_corr_j else col_j\n",
    "                    to_drop.add(col_to_drop)\n",
    "\n",
    "        to_drop_list = list(to_drop)\n",
    "        if not to_drop_list:\n",
    "            print(\"No se encontraron características nuevas altamente correlacionadas para eliminar.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Columnas a eliminar por alta correlación ({len(to_drop_list)}): {to_drop_list}\")\n",
    "        \n",
    "        self.X_train = self.X_train.drop(columns=to_drop_list)\n",
    "        self.X_test = self.X_test.drop(columns=to_drop_list)\n",
    "        \n",
    "        self.cols_to_drop.extend(to_drop_list)\n",
    "        print(\"X_train y X_test actualizados.\")\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        if self.data is None:\n",
    "            print(\"Error: No hay datos cargados. Ejecute .load_data() primero.\")\n",
    "            return self\n",
    "\n",
    "        print(\"\\n\" + \"=\"*30)\n",
    "        print(\"INICIANDO LIMPIEZA Y PREPROCESAMIENTO\")\n",
    "        print(\"=\"*30)\n",
    "        \n",
    "        cleaner = DataCleaner(self.data)\n",
    "        cleaner.filter_expected_columns(self.expected_cols)\n",
    "        cleaner.clean_primary_key(key=\"url\")\n",
    "        cleaner.force_numeric(exclude=[\"url\"])\n",
    "        cleaner.apply_business_rules()\n",
    "        cleaner.normalize_lda(lda_cols=self.lda_cols)\n",
    "        cleaned_data = cleaner.get_df()\n",
    "        print(\"Limpieza de datos estática completada.\")\n",
    "\n",
    "        DataExplorer.generate_profiling_report(\n",
    "            cleaned_data, \n",
    "            \"Reporte 2: Datos Limpios (Post-Limpieza Estática)\", \n",
    "            self.reports_path, \n",
    "            \"02_cleaned_data_report.html\"\n",
    "        )\n",
    "        \n",
    "        X = cleaned_data.drop(self.TARGET_COL, axis=1)\n",
    "        y = cleaned_data[self.TARGET_COL]\n",
    "        y_binary = (y > self.threshold).astype(int)\n",
    "        print(f\"Target '{self.TARGET_COL}' binarizado con umbral > {self.threshold}.\")\n",
    "        \n",
    "        print(\"\\nDividiendo en sets de entrenamiento y prueba...\")\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y_binary, test_size=0.2, random_state=42, stratify=y_binary\n",
    "        )\n",
    "        print(\"Datos divididos exitosamente.\")\n",
    "        \n",
    "        print(f\"\\nEliminando columnas no-features de X_train/X_test: {self.cols_to_drop}\")\n",
    "        self.X_train = self.X_train.drop(self.cols_to_drop, axis=1, errors='ignore')\n",
    "        self.X_test = self.X_test.drop(self.cols_to_drop, axis=1, errors='ignore')\n",
    "        \n",
    "        DataExplorer.plot_correlation_matrix(\n",
    "            self.X_train, \n",
    "            title=\"Matriz de Correlación ANTES de eliminar alta correlación\",\n",
    "            save_path=os.path.join(self.reports_path, \"05_corr_matrix_before.png\")\n",
    "        )\n",
    "        \n",
    "        self._handle_high_correlation(threshold=0.9)\n",
    "        \n",
    "        DataExplorer.plot_correlation_matrix(\n",
    "            self.X_train, \n",
    "            title=\"Matriz de Correlación DESPUÉS de eliminar alta correlación\",\n",
    "            save_path=os.path.join(self.reports_path, \"06_corr_matrix_after.png\")\n",
    "        )\n",
    "        \n",
    "        train_df_report = self.X_train.copy()\n",
    "        train_df_report[self.TARGET_COL] = self.y_train\n",
    "        DataExplorer.generate_profiling_report(\n",
    "            train_df_report, \"Reporte 3: Set de Entrenamiento (Train, Final)\", \n",
    "            self.reports_path, \"03_train_set_report.html\"\n",
    "        )\n",
    "        test_df_report = self.X_test.copy()\n",
    "        test_df_report[self.TARGET_COL] = self.y_test\n",
    "        DataExplorer.generate_profiling_report(\n",
    "            test_df_report, \"Reporte 4: Set de Prueba (Test, Final)\", \n",
    "            self.reports_path, \"04_test_set_report.html\"\n",
    "        )\n",
    "\n",
    "        ##### PIPELINE PREPROCESSING ###\n",
    "        ###\n",
    "        ###\n",
    "\n",
    "        print(\"\\nDefiniendo el Pipeline de preprocesamiento de Scikit-Learn...\")\n",
    "        \n",
    "        numeric_features = self.X_train.select_dtypes(include=np.number).columns\n",
    "        cols_bin, cols_no_bin = classify_numeric_columns(self.X_train[numeric_features])\n",
    "\n",
    "\n",
    "\n",
    "        numeric_non_binary_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('power', PowerTransformer(method='yeo-johnson')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        binary_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent'))\n",
    "        ])\n",
    "\n",
    "        preprocessor = ColumnTransformer(transformers=[\n",
    "            ('num_non_bin', numeric_non_binary_transformer, cols_no_bin),\n",
    "            ('num_bin', binary_transformer, cols_bin)\n",
    "        ], remainder='passthrough')\n",
    "\n",
    "\n",
    "        print(\"Pipeline de preprocesamiento definido exitosamente.\")\n",
    "        print(f\"X_train shape: {self.X_train.shape}, y_train shape: {self.y_train.shape}\")\n",
    "        print(f\"X_test shape: {self.X_test.shape}, y_test shape: {self.y_test.shape}\")\n",
    "        \n",
    "\n",
    "        print(\"=\"*30)\n",
    "        print(\"FIN DE PREPROCESAMIENTO Y DEFINICIÓN DE PIPELINE\")\n",
    "        print(\"=\"*30)\n",
    "\n",
    "        ###\n",
    "        ###\n",
    "        ##### PIPELINE PREPROCESSING ###\n",
    "\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af476aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "========================================\n",
      "INICIANDO FLUJO AUTOMATIZADO\n",
      "========================================\n",
      "Cargando datos desde ../Data/online_news_modified.csv.dvc...\n",
      "Datos cargados exitosamente. Shape=(4, 1)\n",
      "Error al cargar o explorar datos: name 'DataExplorer' is not defined\n",
      "Fallo al cargar datos. Terminando script.\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    filepath = '../Data/online_news_modified.csv.dvc' \n",
    "    \n",
    "    print(\"\\n\\n\" + \"=\"*40)\n",
    "    print(\"INICIANDO FLUJO AUTOMATIZADO\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    model = NewsPopularityModel(filepath)\n",
    "    \n",
    "    if model.load_data() is not None:\n",
    "        model.preprocess_data()\n",
    "        \n",
    "        print(\"\\n...FLUJO AUTOMATIZADO COMPLETADO.\")\n",
    "        print(f\"Verifica la carpeta '{model.reports_path}' para los 4 reportes HTML y 2 imágenes PNG.\")\n",
    "        print(f\"\\nColumnas totales eliminadas del set de features: {model.cols_to_drop}\")\n",
    "    else:\n",
    "        print(\"Fallo al cargar datos. Terminando script.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ca7e62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
